{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OneHiddenLayorNeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-DGOCEsMa0N",
        "colab_type": "text"
      },
      "source": [
        "# One Hidden Layor Neural Network\n",
        "\n",
        "$\\newcommand{\\vect}[1]{\\boldsymbol{#1}} $\n",
        "\n",
        "Given a simple 1 hidden layer neural network of $N$ input and $L$ hidden units and sigmoidal activation everywhere. <br/>\n",
        "Then we have:\n",
        "\\begin{align}\n",
        "f(\\vect{x}, \\theta) &= \n",
        "\\sigma(\\vect{w}^{\\intercal}\n",
        "\\sigma(\\vect{W}^{(1)\\intercal}\n",
        " \\vect{x} + \\vect{b}^{(1)}) + b)  \\\\\n",
        "\\end{align}\n",
        "\n",
        "<br/>\n",
        "\n",
        "The partial derivative for an output weight $w_i$ between hidden unit $i$ and the output unit is given by\n",
        "\n",
        "<br/>\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial J(\\vect{x})}{\\partial w_{i}}\n",
        "&= \\frac{\\partial(f(\\vect{x}) - y)^2}{\\partial w_{i}} \\\\\n",
        "&= 2(f(\\vect{x})-y) \\frac{\\partial f(\\vect{x})}{\\partial w_{i}} \\\\\n",
        "&= 2(f(\\vect{x})-y)f(\\vect{x})(1 - f(\\vect{x}))\n",
        "\\frac{\\partial \\sum_{l=1}^{L}w_lh_l + b}\n",
        "{\\partial w_{i}} \\\\\n",
        "&= 2(f(\\vect{x})-y)f(\\vect{x})(1 - f(\\vect{x})) h_i\\\\\n",
        "\\end{align}\n",
        "\n",
        "<br/>\n",
        "\n",
        "Similiarly, for the output bias weight b, it is given by <br/>\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial J(\\vect{x})}{\\partial b}\n",
        "&= 2(f(\\vect{x})-y)f(\\vect{x})(1 - f(\\vect{x}))\\\\\n",
        "\\end{align}"
      ]
    }
  ]
}